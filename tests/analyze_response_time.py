#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
AIå¯¹è¯å·¥ä½œæµå“åº”æ—¶é—´è¯¦ç»†åˆ†æ
åˆ†ææ¯ä¸ªé˜¶æ®µçš„å…·ä½“è€—æ—¶å’Œç”¨æˆ·æ„ŸçŸ¥
"""

import asyncio
import time
import requests
import json
from dotenv import load_dotenv
import sys
from pathlib import Path

# åŠ è½½ç¯å¢ƒå˜é‡å’Œé¡¹ç›®è·¯å¾„
load_dotenv(".env.local")
project_root = Path(__file__).parent.parent / "backend/joyagent-core/src/main/python"
sys.path.insert(0, str(project_root))

from com.jd.genie.agent.llm.LLMService import llm_service
from com.startup.agents.RequirementAnalysisAgent import RequirementAnalysisAgent

class ResponseTimeAnalyzer:
    """å“åº”æ—¶é—´åˆ†æå™¨"""
    
    def __init__(self):
        self.backend_url = "http://localhost:8080"
        self.stages = []
    
    def log_stage(self, stage_name, duration, description=""):
        """è®°å½•é˜¶æ®µæ—¶é—´"""
        self.stages.append({
            "stage": stage_name,
            "duration": duration,
            "description": description
        })
        print(f"â±ï¸  {stage_name}: {duration:.2f}s {description}")
    
    async def analyze_complete_workflow(self):
        """åˆ†æå®Œæ•´å·¥ä½œæµçš„æ—¶é—´åˆ†å¸ƒ"""
        print("ğŸ” AIå¯¹è¯å·¥ä½œæµ - è¯¦ç»†æ—¶é—´åˆ†æ")
        print("=" * 60)
        
        total_start = time.time()
        
        # é˜¶æ®µ1: ç”¨æˆ·è¾“å…¥å¤„ç† (å‰ç«¯)
        stage1_start = time.time()
        user_input = "æˆ‘æƒ³å¼€å‘ä¸€ä¸ªAIèŠå¤©æœºå™¨äººåº”ç”¨"
        print(f"ğŸ‘¤ ç”¨æˆ·è¾“å…¥: {user_input}")
        stage1_duration = time.time() - stage1_start
        self.log_stage("ç”¨æˆ·è¾“å…¥å¤„ç†", stage1_duration, "(å‰ç«¯UIå“åº”)")
        
        # é˜¶æ®µ2: éœ€æ±‚åˆ†æ (åç«¯LLM)
        stage2_start = time.time()
        agent = RequirementAnalysisAgent()
        analysis_result = await agent.execute({
            "project_description": user_input,
            "analysis_type": "comprehensive"
        })
        stage2_duration = time.time() - stage2_start
        self.log_stage("éœ€æ±‚åˆ†æ", stage2_duration, "(LLMæ·±åº¦åˆ†æ)")
        
        # é˜¶æ®µ3: æ™ºèƒ½ä½“æ¨è (å‰ç«¯é€»è¾‘)
        stage3_start = time.time()
        # æ¨¡æ‹Ÿæ™ºèƒ½ä½“æ¨èé€»è¾‘
        recommended_agents = ["business_canvas_agent", "swot_analysis_agent"]
        stage3_duration = time.time() - stage3_start
        self.log_stage("æ™ºèƒ½ä½“æ¨è", stage3_duration, "(å‰ç«¯é€»è¾‘å¤„ç†)")
        
        # é˜¶æ®µ4: å·¥ä½œæµæ‰§è¡Œ (åç«¯SSE)
        stage4_start = time.time()
        workflow_result = await self.simulate_workflow_execution(user_input)
        stage4_duration = time.time() - stage4_start
        self.log_stage("å·¥ä½œæµæ‰§è¡Œ", stage4_duration, "(SSEæµå¼å“åº”)")
        
        # é˜¶æ®µ5: ç»“æœå±•ç¤º (å‰ç«¯)
        stage5_start = time.time()
        # æ¨¡æ‹Ÿç»“æœå¤„ç†å’Œå±•ç¤º
        await asyncio.sleep(0.1)  # æ¨¡æ‹ŸUIæ›´æ–°æ—¶é—´
        stage5_duration = time.time() - stage5_start
        self.log_stage("ç»“æœå±•ç¤º", stage5_duration, "(å‰ç«¯UIæ›´æ–°)")
        
        total_duration = time.time() - total_start
        
        print("\n" + "=" * 60)
        print("ğŸ“Š æ—¶é—´åˆ†å¸ƒåˆ†æ")
        print("=" * 60)
        
        for stage in self.stages:
            percentage = (stage["duration"] / total_duration) * 100
            bar_length = int(percentage / 2)  # æ¯2%ä¸€ä¸ªå­—ç¬¦
            bar = "â–ˆ" * bar_length + "â–‘" * (50 - bar_length)
            print(f"{stage['stage']:<15} {stage['duration']:>6.2f}s [{bar}] {percentage:>5.1f}%")
        
        print(f"\nâ±ï¸  æ€»è€—æ—¶: {total_duration:.2f}s")
        
        return total_duration
    
    async def simulate_workflow_execution(self, user_input):
        """æ¨¡æ‹Ÿå·¥ä½œæµæ‰§è¡Œï¼Œæµ‹é‡SSEå“åº”æ—¶é—´"""
        try:
            payload = {
                "requestId": f"analysis_{int(time.time())}",
                "query": user_input,
                "agentType": "react",
                "isStream": "true",
                "outputStyle": "markdown"
            }
            
            response = requests.post(
                f"{self.backend_url}/AutoAgent",
                json=payload,
                headers={"Accept": "text/event-stream"},
                stream=True,
                timeout=30
            )
            
            if response.status_code != 200:
                return {"status": "failed", "error": f"HTTP {response.status_code}"}
            
            events_received = []
            first_data_time = None
            last_data_time = None
            
            for line in response.iter_lines(decode_unicode=True):
                if line and line.startswith('data: event: '):
                    event_type = line[13:].strip()
                    current_time = time.time()
                    
                    if first_data_time is None:
                        first_data_time = current_time
                    last_data_time = current_time
                    
                    events_received.append({
                        "event": event_type,
                        "time": current_time
                    })
                    
                    # ç”¨æˆ·æ„ŸçŸ¥çš„å…³é”®æ—¶é—´ç‚¹
                    if event_type == "start":
                        print(f"    ğŸ“¡ é¦–æ¬¡å“åº”: {current_time - payload['requestId'][9:]} ç§’å")
                    elif event_type == "thinking":
                        print(f"    ğŸ¤” å¼€å§‹æ€è€ƒ: ç”¨æˆ·çœ‹åˆ°AIåœ¨æ€è€ƒ")
                    elif event_type == "final_answer":
                        print(f"    âœ… æœ€ç»ˆç­”æ¡ˆ: ç”¨æˆ·çœ‹åˆ°å®Œæ•´ç»“æœ")
                    elif event_type == "complete":
                        print(f"    ğŸ æ‰§è¡Œå®Œæˆ: å·¥ä½œæµç»“æŸ")
                        break
                
                # é™åˆ¶å¤„ç†æ—¶é—´ï¼Œé¿å…æµ‹è¯•è¿‡é•¿
                if len(events_received) >= 10:
                    break
            
            return {
                "status": "completed",
                "events": events_received,
                "first_response_time": first_data_time,
                "total_stream_time": last_data_time - first_data_time if first_data_time and last_data_time else 0
            }
            
        except Exception as e:
            return {"status": "failed", "error": str(e)}
    
    async def analyze_user_perception(self):
        """åˆ†æç”¨æˆ·æ„ŸçŸ¥çš„å“åº”æ—¶é—´"""
        print("\nğŸ§  ç”¨æˆ·æ„ŸçŸ¥åˆ†æ")
        print("=" * 60)
        
        # æµ‹è¯•å³æ—¶å“åº”æ—¶é—´
        instant_start = time.time()
        # æ¨¡æ‹Ÿå‰ç«¯ç«‹å³æ˜¾ç¤º"æ­£åœ¨åˆ†æ..."
        instant_duration = time.time() - instant_start
        print(f"âš¡ å³æ—¶åé¦ˆ: {instant_duration*1000:.1f}ms (ç”¨æˆ·çœ‹åˆ°'æ­£åœ¨åˆ†æ...')")
        
        # æµ‹è¯•é¦–æ¬¡æœ‰æ„ä¹‰å“åº”æ—¶é—´
        first_response_start = time.time()
        simple_response = await llm_service.generate_response(
            "ç®€å•å›ç­”ï¼šAIæ˜¯ä»€ä¹ˆï¼Ÿ",
            max_tokens=50
        )
        first_response_duration = time.time() - first_response_start
        print(f"ğŸ¯ é¦–æ¬¡å“åº”: {first_response_duration:.2f}s (ç”¨æˆ·çœ‹åˆ°ç¬¬ä¸€ä¸ªæœ‰æ„ä¹‰çš„å›ç­”)")
        
        # æµ‹è¯•æµå¼å“åº”çš„æ„ŸçŸ¥
        print(f"ğŸ“Š æµå¼æ„ŸçŸ¥: ç”¨æˆ·åœ¨{first_response_duration:.1f}såå¼€å§‹çœ‹åˆ°å†…å®¹ï¼Œç„¶åæŒç»­æ¥æ”¶æ›´æ–°")
        
        return {
            "instant_feedback": instant_duration,
            "first_meaningful_response": first_response_duration
        }
    
    async def compare_modes(self):
        """æ¯”è¾ƒä¸åŒæ¨¡å¼çš„å“åº”æ—¶é—´"""
        print("\nğŸ”„ æ¨¡å¼å¯¹æ¯”åˆ†æ")
        print("=" * 60)
        
        # ç›´æ¥æ¨¡å¼ (å•æ™ºèƒ½ä½“)
        direct_start = time.time()
        agent = RequirementAnalysisAgent()
        direct_result = await agent.execute({
            "project_description": "ç®€å•æµ‹è¯•",
            "analysis_type": "simple"
        })
        direct_duration = time.time() - direct_start
        print(f"ğŸ’¬ ç›´æ¥æ¨¡å¼: {direct_duration:.2f}s (å•æ™ºèƒ½ä½“å¯¹è¯)")
        
        # ç¼–æ’æ¨¡å¼ (å¤šæ™ºèƒ½ä½“å·¥ä½œæµ)
        orchestrated_start = time.time()
        # æ¨¡æ‹Ÿå®Œæ•´çš„ç¼–æ’æµç¨‹
        await self.analyze_complete_workflow()
        orchestrated_duration = time.time() - orchestrated_start
        print(f"ğŸ¯ ç¼–æ’æ¨¡å¼: {orchestrated_duration:.2f}s (å¤šæ™ºèƒ½ä½“ååŒ)")
        
        print(f"\nğŸ“ˆ æ•ˆç‡å¯¹æ¯”:")
        print(f"   ç›´æ¥æ¨¡å¼é€‚åˆ: ç®€å•é—®ç­”ã€ä¸“ä¸šå’¨è¯¢")
        print(f"   ç¼–æ’æ¨¡å¼é€‚åˆ: å¤æ‚åˆ†æã€å…¨é¢è¯„ä¼°")
        
        return {
            "direct_mode": direct_duration,
            "orchestrated_mode": orchestrated_duration
        }

async def main():
    """ä¸»åˆ†æå‡½æ•°"""
    analyzer = ResponseTimeAnalyzer()
    
    print("ğŸ¯ AIå¯¹è¯å·¥ä½œæµ - å“åº”æ—¶é—´æ·±åº¦åˆ†æ")
    print("ğŸ“‹ åˆ†æç›®æ ‡: ç†è§£ç”¨æˆ·å®é™…æ„ŸçŸ¥çš„å“åº”æ—¶é—´")
    print("â° å¼€å§‹æ—¶é—´:", time.strftime("%Y-%m-%d %H:%M:%S"))
    print("\n")
    
    # å®Œæ•´å·¥ä½œæµåˆ†æ
    total_time = await analyzer.analyze_complete_workflow()
    
    # ç”¨æˆ·æ„ŸçŸ¥åˆ†æ
    perception = await analyzer.analyze_user_perception()
    
    # æ¨¡å¼å¯¹æ¯”
    comparison = await analyzer.compare_modes()
    
    print("\n" + "ğŸ‰" + "=" * 60)
    print("ğŸ’¡ å…³é”®å‘ç°")
    print("=" * 62)
    print(f"1. ç”¨æˆ·å³æ—¶åé¦ˆ: {perception['instant_feedback']*1000:.1f}ms (å‡ ä¹ç¬é—´)")
    print(f"2. é¦–æ¬¡æœ‰æ„ä¹‰å“åº”: {perception['first_meaningful_response']:.2f}s (ç”¨æˆ·å¼€å§‹çœ‹åˆ°å†…å®¹)")
    print(f"3. å®Œæ•´å·¥ä½œæµ: {total_time:.2f}s (åŒ…å«æ·±åº¦åˆ†æ)")
    print(f"4. ç›´æ¥å¯¹è¯æ¨¡å¼: {comparison['direct_mode']:.2f}s (é€‚åˆå¿«é€Ÿé—®ç­”)")
    
    print(f"\nğŸš€ ä¼˜åŒ–å»ºè®®:")
    print(f"   â€¢ ä¿æŒå³æ—¶åé¦ˆ (<100ms)")
    print(f"   â€¢ ä¼˜åŒ–é¦–æ¬¡å“åº” (<3s)")
    print(f"   â€¢ æµå¼æ˜¾ç¤ºè¿‡ç¨‹ (ç”¨æˆ·æŒç»­çœ‹åˆ°è¿›å±•)")
    print(f"   â€¢ æ ¹æ®éœ€æ±‚é€‰æ‹©æ¨¡å¼ (ç®€å•ç”¨ç›´æ¥ï¼Œå¤æ‚ç”¨ç¼–æ’)")
    
    print("\n" + "=" * 62)

if __name__ == "__main__":
    asyncio.run(main())