#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æµ‹è¯•æµå¼å“åº”çš„ç”¨æˆ·æ„ŸçŸ¥
æ¨¡æ‹Ÿç”¨æˆ·å®é™…çœ‹åˆ°å†…å®¹çš„æ—¶é—´ç‚¹
"""

import asyncio
import time
import sys
from pathlib import Path
from dotenv import load_dotenv

# åŠ è½½ç¯å¢ƒå˜é‡å’Œé¡¹ç›®è·¯å¾„
load_dotenv(".env.local")
project_root = Path(__file__).parent.parent / "backend/joyagent-core/src/main/python"
sys.path.insert(0, str(project_root))

from com.jd.genie.agent.llm.LLMService import llm_service

async def test_streaming_perception():
    """æµ‹è¯•æµå¼å“åº”çš„ç”¨æˆ·æ„ŸçŸ¥"""
    print("ğŸ“± æµå¼å“åº”ç”¨æˆ·æ„ŸçŸ¥æµ‹è¯•")
    print("=" * 50)
    
    query = "è¯·ç®€å•ä»‹ç»ä¸€ä¸‹äººå·¥æ™ºèƒ½çš„å‘å±•å†ç¨‹"
    print(f"ğŸ‘¤ ç”¨æˆ·é—®é¢˜: {query}")
    print(f"â° å¼€å§‹æ—¶é—´: {time.strftime('%H:%M:%S')}")
    print("\nğŸ¤– AIå›ç­”è¿‡ç¨‹:")
    print("-" * 30)
    
    start_time = time.time()
    first_chunk_time = None
    meaningful_content_time = None
    chunk_count = 0
    total_content = ""
    
    async for chunk in llm_service.generate_response_stream(
        prompt=query,
        system_prompt="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIåŠ©æ‰‹ï¼Œè¯·ç”¨ç®€æ´æ˜äº†çš„è¯­è¨€å›ç­”é—®é¢˜ã€‚",
        temperature=0.7,
        max_tokens=300
    ):
        current_time = time.time()
        elapsed = current_time - start_time
        
        if chunk and not chunk.startswith("Error:"):
            chunk_count += 1
            total_content += chunk
            
            # è®°å½•é¦–ä¸ªæ•°æ®å—æ—¶é—´
            if first_chunk_time is None:
                first_chunk_time = elapsed
                print(f"âš¡ é¦–ä¸ªæ•°æ®å—: {elapsed:.2f}s")
            
            # è®°å½•æœ‰æ„ä¹‰å†…å®¹æ—¶é—´ï¼ˆè¶…è¿‡10ä¸ªå­—ç¬¦ï¼‰
            if meaningful_content_time is None and len(total_content.strip()) >= 10:
                meaningful_content_time = elapsed
                print(f"ğŸ“ æœ‰æ„ä¹‰å†…å®¹: {elapsed:.2f}s")
            
            # å®æ—¶æ˜¾ç¤ºå†…å®¹ï¼ˆæ¨¡æ‹Ÿç”¨æˆ·çœ‹åˆ°çš„æ•ˆæœï¼‰
            print(chunk, end='', flush=True)
            
            # æ¨¡æ‹Ÿç”¨æˆ·é˜…è¯»æ—¶é—´
            await asyncio.sleep(0.01)
    
    total_time = time.time() - start_time
    
    print(f"\n\n" + "-" * 30)
    print("ğŸ“Š æ„ŸçŸ¥åˆ†æç»“æœ:")
    print(f"   âš¡ é¦–æ¬¡å“åº”: {first_chunk_time:.2f}s (ç”¨æˆ·çœ‹åˆ°AIå¼€å§‹å›ç­”)")
    print(f"   ğŸ“ æœ‰æ„ä¹‰å†…å®¹: {meaningful_content_time:.2f}s (ç”¨æˆ·å¼€å§‹ç†è§£å†…å®¹)")
    print(f"   ğŸ å®Œæ•´å›ç­”: {total_time:.2f}s (ç”¨æˆ·çœ‹åˆ°å®Œæ•´ç­”æ¡ˆ)")
    print(f"   ğŸ“¦ æ•°æ®å—æ•°: {chunk_count} (æµå¼æ›´æ–°æ¬¡æ•°)")
    print(f"   ğŸ“ å†…å®¹é•¿åº¦: {len(total_content)} å­—ç¬¦")
    
    # è®¡ç®—ç”¨æˆ·æ„ŸçŸ¥çš„"ç­‰å¾…æ—¶é—´"
    perceived_wait = meaningful_content_time if meaningful_content_time else first_chunk_time
    print(f"\nğŸ’¡ ç”¨æˆ·æ„ŸçŸ¥ç­‰å¾…æ—¶é—´: {perceived_wait:.2f}s")
    
    if perceived_wait < 2:
        print("   âœ… å“åº”é€Ÿåº¦ä¼˜ç§€ (<2s)")
    elif perceived_wait < 5:
        print("   âš¡ å“åº”é€Ÿåº¦è‰¯å¥½ (2-5s)")
    else:
        print("   âš ï¸  å“åº”é€Ÿåº¦éœ€è¦ä¼˜åŒ– (>5s)")

async def compare_response_modes():
    """æ¯”è¾ƒä¸åŒå“åº”æ¨¡å¼çš„ç”¨æˆ·æ„ŸçŸ¥"""
    print("\nğŸ”„ å“åº”æ¨¡å¼å¯¹æ¯”")
    print("=" * 50)
    
    query = "ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ"
    
    # æµ‹è¯•éæµå¼å“åº”
    print("1ï¸âƒ£ éæµå¼å“åº”:")
    start_time = time.time()
    non_stream_response = await llm_service.generate_response(
        prompt=query,
        max_tokens=100
    )
    non_stream_time = time.time() - start_time
    print(f"   â±ï¸  æ€»æ—¶é—´: {non_stream_time:.2f}s")
    print(f"   ğŸ‘¤ ç”¨æˆ·æ„ŸçŸ¥: ç­‰å¾…{non_stream_time:.2f}såçœ‹åˆ°å®Œæ•´ç­”æ¡ˆ")
    
    # æµ‹è¯•æµå¼å“åº”
    print("\n2ï¸âƒ£ æµå¼å“åº”:")
    start_time = time.time()
    first_chunk_time = None
    chunk_count = 0
    
    async for chunk in llm_service.generate_response_stream(
        prompt=query,
        max_tokens=100
    ):
        if chunk and not chunk.startswith("Error:"):
            chunk_count += 1
            if first_chunk_time is None:
                first_chunk_time = time.time() - start_time
    
    stream_total_time = time.time() - start_time
    print(f"   â±ï¸  é¦–æ¬¡å“åº”: {first_chunk_time:.2f}s")
    print(f"   â±ï¸  æ€»æ—¶é—´: {stream_total_time:.2f}s")
    print(f"   ğŸ‘¤ ç”¨æˆ·æ„ŸçŸ¥: {first_chunk_time:.2f}såå¼€å§‹çœ‹åˆ°å†…å®¹ï¼ŒæŒç»­æ›´æ–°")
    
    # æ„ŸçŸ¥ä¼˜åŠ¿åˆ†æ
    print(f"\nğŸ“ˆ æµå¼å“åº”ä¼˜åŠ¿:")
    if first_chunk_time < non_stream_time:
        improvement = ((non_stream_time - first_chunk_time) / non_stream_time) * 100
        print(f"   âš¡ æ„ŸçŸ¥é€Ÿåº¦æå‡: {improvement:.1f}%")
        print(f"   ğŸ¯ ç”¨æˆ·ä½“éªŒ: æ›´å¿«çš„åé¦ˆï¼Œæ›´å¥½çš„äº¤äº’æ„Ÿ")
    
    print(f"   ğŸ“Š æ•°æ®å—æ•°: {chunk_count} (æ¸è¿›å¼å†…å®¹å±•ç¤º)")

async def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    await test_streaming_perception()
    await compare_response_modes()
    
    print("\n" + "ğŸ‰" + "=" * 50)
    print("ğŸ’¡ å…³é”®æ´å¯Ÿ")
    print("=" * 52)
    print("1. æµå¼å“åº”è®©ç”¨æˆ·åœ¨1-2ç§’å†…å°±èƒ½çœ‹åˆ°AIå¼€å§‹å›ç­”")
    print("2. ç”¨æˆ·ä¸éœ€è¦ç­‰å¾…å®Œæ•´çš„15-18ç§’ï¼Œè€Œæ˜¯æŒç»­çœ‹åˆ°è¿›å±•")
    print("3. å¿ƒç†æ„ŸçŸ¥ï¼š'AIæ­£åœ¨æ€è€ƒå¹¶å›ç­”'è€Œä¸æ˜¯'AIæ²¡æœ‰å“åº”'")
    print("4. 18ç§’çš„æ€»æ—¶é—´è¢«åˆ†è§£ä¸ºå¤šä¸ªçŸ­æš‚çš„ç­‰å¾…æœŸ")
    print("\nğŸš€ ç”¨æˆ·ä½“éªŒä¼˜åŒ–:")
    print("   â€¢ å³æ—¶åé¦ˆ (0.1s): æ˜¾ç¤º'æ­£åœ¨åˆ†æ...'")
    print("   â€¢ å¿«é€Ÿé¦–å“ (1-2s): å¼€å§‹æ˜¾ç¤ºåˆ†æå†…å®¹")
    print("   â€¢ æµå¼æ›´æ–°: æŒç»­æ˜¾ç¤ºåˆ†æè¿›å±•")
    print("   â€¢ é˜¶æ®µæç¤º: å‘ŠçŸ¥ç”¨æˆ·å½“å‰å¤„ç†é˜¶æ®µ")

if __name__ == "__main__":
    asyncio.run(main())