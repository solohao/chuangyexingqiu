#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ¨¡å—1æµ‹è¯•: LLMåŸºç¡€æœåŠ¡
æµ‹è¯•é­”æ­APIé›†æˆã€æµå¼å“åº”ã€é”™è¯¯å¤„ç†
"""

import os
import asyncio
import time
from dotenv import load_dotenv
import sys
from pathlib import Path

# åŠ è½½ç¯å¢ƒå˜é‡å’Œé¡¹ç›®è·¯å¾„
load_dotenv(".env.local")
project_root = Path(__file__).parent.parent / "backend/joyagent-core/src/main/python"
sys.path.insert(0, str(project_root))

from com.jd.genie.agent.llm.LLMService import llm_service

class LLMServiceTester:
    """LLMæœåŠ¡æµ‹è¯•å™¨"""
    
    def __init__(self):
        self.test_results = []
    
    def log_test(self, test_name, success, message, duration=None):
        """è®°å½•æµ‹è¯•ç»“æœ"""
        status = "âœ… PASS" if success else "âŒ FAIL"
        duration_str = f" ({duration:.2f}s)" if duration else ""
        print(f"{status} {test_name}{duration_str}: {message}")
        self.test_results.append({
            "test": test_name,
            "success": success,
            "message": message,
            "duration": duration
        })
    
    async def test_service_initialization(self):
        """æµ‹è¯•1.1: æœåŠ¡åˆå§‹åŒ–"""
        start_time = time.time()
        try:
            api_key = os.getenv("MODELSCOPE_ACCESS_TOKEN")
            if not api_key:
                self.log_test("æœåŠ¡åˆå§‹åŒ–", False, "ç¯å¢ƒå˜é‡MODELSCOPE_ACCESS_TOKENæœªè®¾ç½®")
                return False
            
            if not llm_service.client:
                self.log_test("æœåŠ¡åˆå§‹åŒ–", False, "LLMå®¢æˆ·ç«¯æœªæ­£ç¡®åˆå§‹åŒ–")
                return False
            
            self.log_test("æœåŠ¡åˆå§‹åŒ–", True, f"API Key: {api_key[:10]}...", time.time() - start_time)
            return True
        except Exception as e:
            self.log_test("æœåŠ¡åˆå§‹åŒ–", False, f"åˆå§‹åŒ–å¼‚å¸¸: {e}", time.time() - start_time)
            return False
    
    async def test_simple_completion(self):
        """æµ‹è¯•1.2: ç®€å•æ–‡æœ¬ç”Ÿæˆ"""
        start_time = time.time()
        try:
            response = await llm_service.generate_response(
                prompt="1+1ç­‰äºå¤šå°‘ï¼Ÿ",
                system_prompt="ä½ æ˜¯ä¸€ä¸ªæ•°å­¦åŠ©æ‰‹ï¼Œè¯·ç®€æ´å›ç­”ã€‚",
                temperature=0.1,
                max_tokens=50
            )
            
            if response.startswith("Error:"):
                self.log_test("ç®€å•æ–‡æœ¬ç”Ÿæˆ", False, response, time.time() - start_time)
                return False
            
            if "2" in response:
                self.log_test("ç®€å•æ–‡æœ¬ç”Ÿæˆ", True, f"å“åº”: {response[:50]}...", time.time() - start_time)
                return True
            else:
                self.log_test("ç®€å•æ–‡æœ¬ç”Ÿæˆ", False, f"å“åº”ä¸æ­£ç¡®: {response}", time.time() - start_time)
                return False
                
        except Exception as e:
            self.log_test("ç®€å•æ–‡æœ¬ç”Ÿæˆ", False, f"ç”Ÿæˆå¼‚å¸¸: {e}", time.time() - start_time)
            return False
    
    async def test_stream_completion(self):
        """æµ‹è¯•1.3: æµå¼æ–‡æœ¬ç”Ÿæˆ"""
        start_time = time.time()
        try:
            chunks = []
            async for chunk in llm_service.generate_response_stream(
                prompt="è¯·ç®€å•ä»‹ç»Pythonç¼–ç¨‹è¯­è¨€",
                system_prompt="ä½ æ˜¯ä¸€ä¸ªç¼–ç¨‹åŠ©æ‰‹ã€‚",
                temperature=0.7,
                max_tokens=200
            ):
                if chunk.startswith("Error:"):
                    self.log_test("æµå¼æ–‡æœ¬ç”Ÿæˆ", False, chunk, time.time() - start_time)
                    return False
                chunks.append(chunk)
            
            full_response = "".join(chunks)
            if len(chunks) > 1 and "Python" in full_response:
                self.log_test("æµå¼æ–‡æœ¬ç”Ÿæˆ", True, f"å—æ•°: {len(chunks)}, é•¿åº¦: {len(full_response)}", time.time() - start_time)
                return True
            else:
                self.log_test("æµå¼æ–‡æœ¬ç”Ÿæˆ", False, f"æµå¼å“åº”å¼‚å¸¸: å—æ•°={len(chunks)}", time.time() - start_time)
                return False
                
        except Exception as e:
            self.log_test("æµå¼æ–‡æœ¬ç”Ÿæˆ", False, f"æµå¼å¼‚å¸¸: {e}", time.time() - start_time)
            return False
    
    async def test_chat_completion(self):
        """æµ‹è¯•1.4: èŠå¤©å®Œæˆæ¥å£"""
        start_time = time.time()
        try:
            messages = [
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªå‹å¥½çš„åŠ©æ‰‹ã€‚"},
                {"role": "user", "content": "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±ã€‚"}
            ]
            
            result = await llm_service.chat_completion(
                messages=messages,
                temperature=0.7,
                max_tokens=100,
                stream=False
            )
            
            if "error" in result:
                self.log_test("èŠå¤©å®Œæˆæ¥å£", False, result["error"], time.time() - start_time)
                return False
            
            if "choices" in result and result["choices"][0]["message"]["content"]:
                content = result["choices"][0]["message"]["content"]
                self.log_test("èŠå¤©å®Œæˆæ¥å£", True, f"å“åº”é•¿åº¦: {len(content)}", time.time() - start_time)
                return True
            else:
                self.log_test("èŠå¤©å®Œæˆæ¥å£", False, "å“åº”æ ¼å¼é”™è¯¯", time.time() - start_time)
                return False
                
        except Exception as e:
            self.log_test("èŠå¤©å®Œæˆæ¥å£", False, f"èŠå¤©å¼‚å¸¸: {e}", time.time() - start_time)
            return False
    
    async def test_error_handling(self):
        """æµ‹è¯•1.5: é”™è¯¯å¤„ç†"""
        start_time = time.time()
        try:
            # æµ‹è¯•æ— æ•ˆæ¨¡å‹
            response = await llm_service.generate_response(
                prompt="æµ‹è¯•",
                model="invalid-model-name",
                max_tokens=10
            )
            
            if response.startswith("Error:"):
                self.log_test("é”™è¯¯å¤„ç†", True, "æ­£ç¡®å¤„ç†æ— æ•ˆæ¨¡å‹é”™è¯¯", time.time() - start_time)
                return True
            else:
                self.log_test("é”™è¯¯å¤„ç†", False, "æœªæ­£ç¡®å¤„ç†é”™è¯¯", time.time() - start_time)
                return False
                
        except Exception as e:
            self.log_test("é”™è¯¯å¤„ç†", True, f"æ­£ç¡®æŠ›å‡ºå¼‚å¸¸: {type(e).__name__}", time.time() - start_time)
            return True
    
    async def run_all_tests(self):
        """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
        print("ğŸš€ æ¨¡å—1æµ‹è¯•: LLMåŸºç¡€æœåŠ¡")
        print("=" * 60)
        
        tests = [
            self.test_service_initialization,
            self.test_simple_completion,
            self.test_stream_completion,
            self.test_chat_completion,
            self.test_error_handling
        ]
        
        passed = 0
        total = len(tests)
        
        for test in tests:
            if await test():
                passed += 1
        
        print("\n" + "=" * 60)
        print(f"ğŸ“Š æµ‹è¯•ç»“æœ: {passed}/{total} é€šè¿‡")
        
        if passed == total:
            print("ğŸ‰ æ¨¡å—1: LLMåŸºç¡€æœåŠ¡ - å…¨éƒ¨æµ‹è¯•é€šè¿‡ï¼")
        else:
            print("âš ï¸  æ¨¡å—1: LLMåŸºç¡€æœåŠ¡ - å­˜åœ¨å¤±è´¥çš„æµ‹è¯•")
        
        return passed == total

async def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    tester = LLMServiceTester()
    success = await tester.run_all_tests()
    return success

if __name__ == "__main__":
    asyncio.run(main())